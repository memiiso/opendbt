{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#opendbt","title":"opendbt","text":"<p>This project adds new capabilities to dbt-core by dynamically extending dbt's source code.</p> <p>dbt is a popular solution for batch data processing in data analytics. While it operates on an open-core model, which can sometimes limit the inclusion of community features in the open-source version. no worries opendbt is here to solve it. opendbt offers a fully open-source package to address these concerns. OpenDBT builds upon dbt-core, adding valuable features without changing dbt-core code.</p> <p>With <code>opendbt</code> you can go beyond the core functionalities of dbt. For example seamlessly integrating your customized adapter and providing jinja context with further adapter/python methods.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> Includes superior dbt catalog UI, user-friendly   multi-project   data catalog,   including row level   lineage, see it here</li> <li> Integrates Python and DLT Jobs to dbt. Enables Extract&amp;Load (EL) with dbt.</li> <li> Supports DBT Mesh setups. Supports running multiple projects which are using cross project ref   models.</li> <li> And many more features, customization options.</li> <li>Customize Existing Adapters: add your custom logic to current adapters</li> <li>By extending current adapter provide more functions to jinja</li> <li>Execute Local Python     Code: run local Python code. For example, you     could import data from web APIs directly within your dbt model.</li> <li>Integrate DLT. Run end to end ETL pipeline with dbt and DLT.</li> <li>Use multi project dbt-mesh setup cross-project references.<ul> <li>This feature was only available in \"dbt Cloud Enterprise\" so far.</li> </ul> </li> <li>Granular Model-Level Orchestration with Airflow: Integrate Airflow for fine-grained control over model execution.</li> <li>Serve dbt Docs in Airflow UI: Create a custom page on the Airflow server that displays dbt documentation as an     Airflow UI page. Supports both single-project and multi-project configurations with UI-based project switching.</li> <li>Register dbt callbacks within a     dbt project to trigger custom actions or alerting based on selected dbt events.</li> </ul> <p>See documentation for further details and detailed examples.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>install from github or pypi:</p> <pre><code>pip install opendbt==0.13.0\n# Or\npip install https://github.com/memiiso/opendbt/archive/refs/tags/0.4.0.zip --upgrade --user\n</code></pre>"},{"location":"#your-contributions-matter","title":"Your Contributions Matter","text":"<p>The project completely open-source, using the Apache 2.0 license. opendbt still is a young project and there are things to improve. Please feel free to test it, give feedback, open feature requests or send pull requests.</p>"},{"location":"#contributors","title":"Contributors","text":""},{"location":"adapter_integration/","title":"Using a Custom Adapter","text":"<p>opendbt provides the flexibility to register and utilize custom adapters. This capability allows users to extend existing adapters, add new methods, or override existing ones. By introducing new methods to an adapter, you can expose them to dbt macros and leverage them within your dbt models, for instance, by calling a new method during materialization.</p> <p>Let's walk through the process step by step.</p> <p>lets see it ste by step</p>"},{"location":"adapter_integration/#1-extend-existing-adapter","title":"1: Extend Existing Adapter","text":"<p>Create a new adapter class that inherits from the desired base adapter. Add the necessary methods to this class.</p> <p>here we are creating a method which will run given pyhon model locally. Notice <code>@available</code> decorator is making it available for use in dbt macros.</p> <pre><code>import importlib\nimport logging\nimport sys\nimport tempfile\nfrom multiprocessing.context import SpawnContext\nfrom typing import Dict\n\nfrom dbt.adapters.base import available\nfrom dbt.adapters.duckdb import DuckDBAdapter\n\n\nclass DuckDBAdapterV2Custom(DuckDBAdapter):\n    def __init__(self, config, mp_context: SpawnContext = None) -&gt; None:\n        print(f\"WARNING: Using User Provided DBT Adapter: {type(self).__module__}.{type(self).__name__}\")\n        # pylint: disable=no-value-for-parameter\n        if mp_context:\n            # DBT 1.8 and above\n            super().__init__(config=config, mp_context=mp_context)\n        else:\n            # DBT 1.7\n            super().__init__(config=config)\n\n    def _execute_python_model(self, model_name: str, compiled_code: str, **kwargs):\n        try:\n            with tempfile.NamedTemporaryFile(suffix=f'.py', delete=True) as model_file:\n                try:\n                    model_file.write(compiled_code.lstrip().encode('utf-8'))\n                    model_file.flush()\n                    print(f\"Created temp py file {model_file.name}\")\n                    # Load the module spec\n                    spec = importlib.util.spec_from_file_location(model_name, model_file.name)\n                    # Create a module object\n                    module = importlib.util.module_from_spec(spec)\n                    # Load the module\n                    sys.modules[model_name] = module\n                    spec.loader.exec_module(module)\n                    dbt_obj = module.dbtObj(None)\n                    # Access and call `model` function of the model!\n                    # IMPORTANT: here we are passing down dbt connection object from the adapter to the model\n                    module.model(dbt=dbt_obj, **kwargs)\n                except Exception as e:\n                    raise Exception(\n                        f\"Failed to load or execute python model:{model_name} from file {model_file.as_posix()}\") from e\n                finally:\n                    model_file.close()\n        except Exception as e:\n            raise Exception(f\"Failed to create temp py file for model:{model_name}\") from e\n\n    @available\n    def submit_local_python_job(self, parsed_model: Dict, compiled_code: str):\n        connection = self.connections.get_if_exists()\n        if not connection:\n            connection = self.connections.get_thread_connection()\n</code></pre>"},{"location":"adapter_integration/#2-activate-custom-adapter","title":"2: Activate Custom Adapter","text":"<p>In your <code>dbt_project.yml</code> file, set the <code>dbt_custom_adapter</code> variable to the fully qualified name of your custom adapter class. when defined opendbt will take this adapter and activates it.</p> <pre><code>vars:\n  dbt_custom_adapter: opendbt.examples.DuckDBAdapterV2Custom\n</code></pre> <p>Optionally you could provide this with run command</p> <pre><code>from opendbt import OpenDbtProject\n\ndp = OpenDbtProject(project_dir=\"/dbt/project_dir\", profiles_dir=\"/dbt/profiles_dir\",\n                    args=['--vars', 'dbt_custom_adapter: opendbt.examples.DuckDBAdapterV2Custom'])\ndp.run(command=\"run\", args=['--select', 'my_executedlt_model'])\n</code></pre>"},{"location":"adapter_integration/#3-use-new-adapter-in-dbt-macro","title":"3: Use new adapter in dbt macro","text":"<p>Call new adapter method from dbt macro/model.</p> <pre><code>  {% call noop_statement(name='main', message='Executed Python', code=compiled_code, rows_affected=-1, res=None) %}\n      {%- set res = adapter.submit_local_python_job(model, compiled_code) -%}\n  {% endcall %}\n</code></pre>"},{"location":"adapter_integration/#4-final","title":"4: Final","text":"<p>Execute dbt commands as usual. dbt will now load and utilize your custom adapter class, allowing you to access the newly defined methods within your Jinja macros.</p> <pre><code>from opendbt import OpenDbtProject\n\ndp = OpenDbtProject(project_dir=\"/dbt/project_dir\", profiles_dir=\"/dbt/profiles_dir\")\ndp.run(command=\"run\")\n</code></pre>"},{"location":"catalog/","title":"Opendbt Catalog","text":"<p>See it in action</p>"},{"location":"catalog/#generating-the-enhanced-catalog","title":"Generating the Enhanced Catalog","text":"<p>To generate the enhanced catalog UI with column-level lineage, use the opendbt CLI:</p> <pre><code>python -m opendbt docs generate\n</code></pre> <p>This command will: - Generate standard dbt catalog files (<code>catalog.json</code>, <code>manifest.json</code>) - Generate <code>catalogl.json</code> with column-level lineage information - Deploy the enhanced catalog UI to your target directory</p> <p>Important: Using the standard <code>dbt docs generate</code> command will generate only the basic dbt documentation without the enhanced UI and <code>catalogl.json</code> file.</p>"},{"location":"catalog/#customizing-the-catalog-ui","title":"Customizing the Catalog UI","text":"<p>You can override the default enhanced UI with your own custom <code>index.html</code> by placing it in your dbt project's <code>docs</code> directory:</p> <ol> <li>Create a <code>docs</code> folder in your dbt project root (if it doesn't exist)</li> <li>Add your custom <code>index.html</code> to this folder</li> <li>Ensure <code>dbt_project.yml</code> includes the docs path:    <pre><code>docs-paths: [\"docs\"]\n</code></pre></li> <li>Run <code>python -m opendbt docs generate</code></li> </ol> <p>The priority order is: 1. User-provided <code>index.html</code> from <code>docs-paths</code> (highest priority) 2. Opendbt's enhanced catalog UI (automatic fallback) 3. Standard dbt-generated UI (only if using <code>dbt docs generate</code>)</p>"},{"location":"catalog/#airflow-integration","title":"Airflow Integration","text":""},{"location":"catalog/#configuration","title":"Configuration","text":"<p>The plugin auto-detects single/multi-project mode. If Airflow Variable <code>opendbt_docs_projects</code> is set, it overrides paths in the code.</p>"},{"location":"catalog/#using-airflow-variable","title":"Using Airflow Variable","text":"<pre><code># Single project\nairflow variables set opendbt_docs_projects '\"/opt/dbtcore/target\"'\n\n# Multiple projects\nairflow variables set opendbt_docs_projects '[\"/opt/dbtcore/target\", \"/opt/dbtfinance/target\"]'\n</code></pre>"},{"location":"catalog/#using-hardcoded-paths","title":"Using hardcoded paths","text":"<pre><code>from pathlib import Path\nfrom opendbt.airflow import plugin\n\n# Single project\nairflow_dbtdocs_page = plugin.init_plugins_dbtdocs_page(\n    Path('/opt/dbtcore/target')\n)\n\n# Multiple projects\nairflow_dbtdocs_page = plugin.init_plugins_dbtdocs_page([\n    Path('/opt/dbtcore/target'),\n    Path('/opt/dbtfinance/target')\n])\n</code></pre> <p>Project names are extracted from parent directory (e.g., <code>/opt/dbtcore/target</code> \u2192 <code>dbtcore</code>).</p> <p>Access docs at <code>/dbt/dbt_docs_index.html</code>. For multiple projects, use <code>?project=&lt;name&gt;</code> or the dropdown selector.</p> <p>If some users can't see DBT Docs tab, add \"menu access on DBT Docs\" to required roles in Security - List Roles.</p>"},{"location":"catalog/#catalog-files-summary","title":"Catalog Files Summary","text":"<ul> <li>catalog.json: Generated by dbt</li> <li>catalogl.json: Generated by opendbt contains extended catalog information with column level lineage</li> <li>manifest.json: Generated by dbt</li> <li>run_info.json: Generated by opendbt, contains latest run information per object/model</li> </ul>"},{"location":"catalog/#key-features","title":"Key Features","text":""},{"location":"catalog/#up-to-date-run-information","title":"Up to date Run information","text":""},{"location":"catalog/#run-information-with-error-messages","title":"Run information with error messages","text":""},{"location":"catalog/#model-dependencies-including-tests","title":"Model dependencies including tests","text":""},{"location":"catalog/#column-level-dependency-lineage-transformation","title":"Column level dependency lineage, transformation","text":""},{"location":"catalog/#dependency-lineage","title":"Dependency lineage","text":""},{"location":"catalog/#multi-project-support","title":"Multi-project support","text":""},{"location":"dlt_integration/","title":"Using DLT within DBT model","text":""},{"location":"dlt_integration/#1-local-python-execution","title":"1: Local Python Execution","text":"<p>Ensure you are able to run local python model.</p>"},{"location":"dlt_integration/#2-create-dlt-model","title":"2: Create DLT Model","text":"<p>To integrate DLT (Data Load Tool) into your dbt model, you'll first create a DLT pipeline. Key parameters for this pipeline, such as the destination, dataset_name, and table_name, can be dynamically configured using dbt variables.</p> <p>You'll then need to define a custom method responsible for ingesting the source data that feeds into the DLT pipeline. The example code demonstrates this with an events() method.</p> <p>It's important to note that the DLT destination is typically derived from your existing dbt connection profile. While the specific construction of the DLT destination might vary depending on the target database (e.g., Snowflake, BigQuery, DuckDB), your dbt connection provides the necessary details to establish it in all scenarios. This allows for a consistent approach to configuring DLT pipelines within your dbt projects.</p> <pre><code>import dlt\n\n@dlt.resource(\n    columns={\"event_tstamp\": {\"data_type\": \"timestamp\", \"precision\": 3}},\n    primary_key=\"event_id\",\n)\ndef events():\n    yield [{\"event_id\": 1, \"event_tstamp\": \"2024-07-30T10:00:00.123\"},\n           {\"event_id\": 2, \"event_tstamp\": \"2025-02-30T10:00:00.321\"}]\n\n\ndef model(dbt, connection: \"Connection\"):\n    \"\"\"\n\n    :param dbt:\n    :param connection: dbt connection to target database\n    :return:\n    \"\"\"\n\n    dbt.config(materialized=\"executepython\")\n\n    parts = [p.strip('\"') for p in str(dbt.this).split('.')]\n    database, schema, table_name = parts[0], parts[1], parts[2]\n    pipeline_name = f\"{database}_{schema}_{table_name}\".replace(' ', '_')\n\n    # IMPORTANT: here we are using dbt connection and mapping it to dlt destination\n    # this might differ for each database\n    dlt_destination = dlt.destinations.duckdb(connection.handle._env.conn)\n\n    # IMPORTANT: here we are configuring and preparing dlt.pipeline for the model!\n    pipeline = dlt.pipeline(\n        pipeline_name=pipeline_name,\n        destination=dlt_destination,\n        dataset_name=schema,\n        dev_mode=False,\n    )\n    print(\"========================================================\")\n    print(f\"INFO: DLT Pipeline pipeline_name:{pipeline.pipeline_name}\")\n    print(f\"INFO: DLT Pipeline dataset_name:{pipeline.dataset_name}\")\n    print(f\"INFO: DLT Pipeline dataset_name:{pipeline}\")\n    print(f\"INFO: DLT Pipeline staging:{pipeline.staging}\")\n    print(f\"INFO: DLT Pipeline destination:{pipeline.destination}\")\n    print(f\"INFO: DLT Pipeline _pipeline_storage:{pipeline._pipeline_storage}\")\n    print(f\"INFO: DLT Pipeline _schema_storage:{pipeline._schema_storage}\")\n    print(f\"INFO: DLT Pipeline state:{pipeline.state}\")\n    print(f\"INFO: DBT this:{dbt.this}\")\n    print(\"========================================================\")\n    load_info = pipeline.run(events(), dataset_name=schema, table_name=table_name)\n    print(load_info)\n    row_counts = pipeline.last_trace.last_normalize_info\n    print(row_counts)\n    print(\"========================================================\")\n    return None\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#orchestrating-dbt-models-with-airflow","title":"Orchestrating dbt Models with Airflow","text":"<p>Step-1: Let's create an Airflow DAG to orchestrate the execution of your dbt project.</p> <pre><code>with DAG(\n        dag_id='dbt_workflow',\n        default_args=default_args,\n        description='DAG To run dbt',\n        schedule_interval=None,\n        start_date=days_ago(3),\n        catchup=False,\n        max_active_runs=1\n) as dag:\n    start = EmptyOperator(task_id=\"start\")\n    end = EmptyOperator(task_id=\"end\")\n\n    DBT_PROJ_DIR = Path(\"/opt/dbtcore\")\n\n    p = OpenDbtAirflowProject(project_dir=DBT_PROJ_DIR, profiles_dir=DBT_PROJ_DIR, target='dev')\n    p.load_dbt_tasks(dag=dag, start_node=start, end_node=end, run_tests_after_all_models=False, include_singular_tests=True, include_dbt_seeds=True)\n</code></pre> <p></p>"},{"location":"examples/#creating-dag-to-run-dbt-tests-after-all-models","title":"Creating dag to run dbt tests after all models","text":"<pre><code>from opendbt.airflow import OpenDbtAirflowProject\n\np = OpenDbtAirflowProject(project_dir=\"/dbt/project_dir\", profiles_dir=\"/dbt/profiles_dir\",\n                        target='dev')\np.load_dbt_tasks(dag=dag, start_node=start, end_node=end, run_tests_after_all_models=True)\n</code></pre>"},{"location":"examples/#creating-airflow-dag-that-selectively-executes-a-specific-subset-of-models-from-your-dbt-project","title":"Creating Airflow DAG that selectively executes a specific subset of models from your dbt project.","text":"<pre><code>from opendbt.airflow import OpenDbtAirflowProject\n\n# create dbt build tasks for models with given tag\np = OpenDbtAirflowProject(resource_type='model', project_dir=\"/dbt/project_dir\", profiles_dir=\"/dbt/profiles_dir\",\n                          target='dev', tag=\"MY_TAG\")\np.load_dbt_tasks(dag=dag, start_node=start, end_node=end)\n</code></pre>"},{"location":"examples/#creating-dag-to-run-only-dbt-tests","title":"Creating dag to run only dbt tests","text":"<pre><code>from opendbt.airflow import OpenDbtAirflowProject\n\n# create dbt test tasks with given model tag\np = OpenDbtAirflowProject(resource_type='test', project_dir=\"/dbt/project_dir\", profiles_dir=\"/dbt/profiles_dir\",\n                          target='dev', tag=\"MY_TAG\")\np.load_dbt_tasks(dag=dag, start_node=start, end_node=end)\n</code></pre>"},{"location":"examples/#integrating-dbt-documentation-into-airflow","title":"Integrating dbt Documentation into Airflow","text":"<p>Airflow, a powerful workflow orchestration tool, can be leveraged to streamline not only dbt execution but also dbt documentation access. By integrating dbt documentation into your Airflow interface, you can centralize your data engineering resources and improve team collaboration.</p> <p>here is how: Step-1: Create python file. Navigate to your Airflow's <code>{airflow}/plugins</code> directory. Create a new Python file and name it appropriately, such as <code>dbt_docs_plugin.py</code>. Add following code to <code>dbt_docs_plugin.py</code> file. Ensure that the specified path accurately points to the folder where your dbt project generates its documentation. https://github.com/memiiso/opendbt/blob/main/tests/resources/airflow/plugins/airflow_dbtdocs_page.py#L1</p> <p>Step-2: Restart Airflow to activate the plugin. Once the restart is complete, you should see a new link labeled <code>DBT Docs</code> within your Airflow web interface. This link will provide access to your dbt documentation. </p> <p>Step-3: Click on the <code>DBT Docs</code> link to open your dbt documentation. </p> <p>Step-4: To use UI of Data Catalog (Demo), run command and reload the page: <pre><code>python -m opendbt docs generate\n</code></pre> </p>"},{"location":"python_integration/","title":"Executing Python Models Locally","text":"<p>You can extend dbt to execute Python code locally by utilizing a customized adapter in conjunction with a custom materialization.</p> <p>This approach offers a powerful way to integrate tasks like data ingestion from external APIs directly into your dbt workflows. It allows for the development of end-to-end data ingestion pipelines entirely within the dbt framework.</p> <p>NOTE: Be aware that this method means Python code execution and data processing occur within the dbt environment itself. For instance, if dbt is deployed on an Airflow server, the processing happens on that server. While this is generally fine for handling reasonable amounts of data, it might not be suitable for very large datasets or resource-intensive processing tasks.</p>"},{"location":"python_integration/#1-extend-adapter","title":"1: Extend Adapter","text":"<p>Opendbt already comes with this feature implemented!</p> <p>Below <code>submit_local_python_job</code> method will execute the provided Python code(compiled Python model) as a subprocess. Note the <code>connection</code> variable passed down to the python model. Which is used to save data to destination database.</p> <pre><code>import importlib\nimport sys\nimport tempfile\nfrom typing import Dict\n\nfrom dbt.adapters.base import available, BaseAdapter\n\nfrom opendbt.runtime_patcher import PatchClass\n\n\n@PatchClass(module_name=\"dbt.adapters.base\", target_name=\"BaseAdapter\")\nclass OpenDbtBaseAdapter(BaseAdapter):\n\n    def _execute_python_model(self, model_name: str, compiled_code: str, **kwargs):\n        try:\n            with tempfile.NamedTemporaryFile(suffix=f'.py', delete=True) as model_file:\n                try:\n                    model_file.write(compiled_code.lstrip().encode('utf-8'))\n                    model_file.flush()\n                    print(f\"Created temp py file {model_file.name}\")\n                    # Load the module spec\n                    spec = importlib.util.spec_from_file_location(model_name, model_file.name)\n                    # Create a module object\n                    module = importlib.util.module_from_spec(spec)\n                    # Load the module\n                    sys.modules[model_name] = module\n                    spec.loader.exec_module(module)\n                    dbt_obj = module.dbtObj(None)\n                    # Access and call `model` function of the model!\n                    # IMPORTANT: here we are passing down duckdb session from the adapter to the model\n                    module.model(dbt=dbt_obj, **kwargs)\n                except Exception as e:\n                    raise Exception(\n                        f\"Failed to load or execute python model:{model_name} from file {model_file.name} due to: {e!r}\") from e\n                finally:\n                    model_file.close()\n        except Exception as e:\n            raise Exception(f\"Failed to create temp py file for model:{model_name} due to: {e!r}\") from e\n\n    @available\n    def submit_local_python_job(self, parsed_model: Dict, compiled_code: str):\n        connection = self.connections.get_if_exists()\n        if not connection:\n            connection = self.connections.get_thread_connection()\n        self._execute_python_model(model_name=parsed_model['name'],\n                                   compiled_code=compiled_code,\n                                   # following args passed to model\n                                   connection=connection)\n</code></pre>"},{"location":"python_integration/#2-pyhon-execution-from-macro","title":"2: Pyhon Execution from macro","text":"<p>Create a new materialization named <code>executepython</code>. This materialization will call the newly added <code>submit_local_python_job</code> method to execute the compiled Python code.</p> <pre><code>  {% call noop_statement(name='main', message='Executed Python', code=compiled_code, rows_affected=-1, res=None) %}\n      {%- set res = adapter.submit_local_python_job(model, compiled_code) -%}\n  {% endcall %}\n</code></pre>"},{"location":"python_integration/#3-final","title":"3: Final","text":"<p>Let's create a sample Python model that will be executed locally by dbt using the <code>executepython</code> materialization.</p> <pre><code>import os\nimport platform\n\nfrom dbt import version\n\n\ndef print_info():\n    _str = f\"name:{os.name}, system:{platform.system()} release:{platform.release()}\"\n    _str += f\"\\npython version:{platform.python_version()}, dbt:{version.__version__}\"\n    print(_str)\n\n\ndef model(dbt, connection: \"Connection\"):\n    dbt.config(materialized=\"executepython\")\n    print(\"==================================================\")\n    print(\"========IM LOCALLY EXECUTED PYTHON MODEL==========\")\n    print(\"==================================================\")\n    print_info()\n    print(\"==================================================\")\n    print(\"===============MAKE DBT GREAT AGAIN===============\")\n    print(\"==================================================\")\n    return None\n</code></pre>"}]}